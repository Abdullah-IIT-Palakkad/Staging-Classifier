{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Unet Feature Extractor.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0blm0uWqv0B"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import lr_scheduler\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class DoubleConv(nn.Module):\n",
        "    \"\"\"(convolution => [BN] => ReLU) * 2\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, mid_channels=None):\n",
        "        super().__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.double_conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(mid_channels),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.double_conv(x)\n",
        "\n",
        "\n",
        "class Down(nn.Module):\n",
        "    \"\"\"Downscaling with maxpool then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super().__init__()\n",
        "        self.maxpool_conv = nn.Sequential(\n",
        "            nn.MaxPool2d(2),\n",
        "            DoubleConv(in_channels, out_channels)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.maxpool_conv(x)\n",
        "\n",
        "\n",
        "class Up(nn.Module):\n",
        "    \"\"\"Upscaling then double conv\"\"\"\n",
        "\n",
        "    def __init__(self, in_channels, out_channels, bilinear=True):\n",
        "        super().__init__()\n",
        "\n",
        "        # if bilinear, use the normal convolutions to reduce the number of channels\n",
        "        if bilinear:\n",
        "            self.up = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
        "            self.conv = DoubleConv(in_channels, out_channels, in_channels // 2)\n",
        "        else:\n",
        "            self.up = nn.ConvTranspose2d(in_channels , in_channels // 2, kernel_size=2, stride=2)\n",
        "            self.conv = DoubleConv(in_channels, out_channels)\n",
        "\n",
        "\n",
        "    def forward(self, x1, x2):\n",
        "        x1 = self.up(x1)\n",
        "        # input is CHW\n",
        "        diffY = x2.size()[2] - x1.size()[2]\n",
        "        diffX = x2.size()[3] - x1.size()[3]\n",
        "\n",
        "        x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
        "                        diffY // 2, diffY - diffY // 2])\n",
        "        x = torch.cat([x2, x1], dim=1)\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class OutConv(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels):\n",
        "        super(OutConv, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "class Down_batch_mid(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, mid_channels = None):\n",
        "        super(Down_batch_mid, self).__init__()\n",
        "        if not mid_channels:\n",
        "            mid_channels = out_channels\n",
        "        self.conv1 = nn.Conv2d(in_channels, mid_channels, kernel_size=3, padding=1)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(mid_channels)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = nn.Conv2d(mid_channels, out_channels, kernel_size=3, padding=1)\n",
        "        self.batchnorm2 = nn.BatchNorm2d(out_channels)\n",
        "        self.maxpool = nn.MaxPool2d(2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        output = self.maxpool(x)\n",
        "        output = self.conv1(output)\n",
        "        output = self.batchnorm1(output)\n",
        "        output = self.relu(output)\n",
        "        latent_variable = self.conv2(output)\n",
        "        output = self.batchnorm2(latent_variable)\n",
        "        output = self.relu(output)\n",
        "        return output, latent_variable\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, n_channels, n_classes, bilinear=True):\n",
        "        super(UNet, self).__init__()\n",
        "        self.n_channels = n_channels\n",
        "        self.n_classes = n_classes\n",
        "        self.bilinear = bilinear\n",
        "\n",
        "        self.inc = DoubleConv(n_channels, 64)\n",
        "        self.down1 = Down(64, 128)\n",
        "        self.down2 = Down(128, 256)\n",
        "        self.down3 = Down(256, 512)\n",
        "        factor = 2 if bilinear else 1\n",
        "        self.down4 = Down_batch_mid(512, 1024 // factor)\n",
        "\n",
        "        self.up1 = Up(1024, 512 // factor, bilinear)\n",
        "        self.up2 = Up(512, 256 // factor, bilinear)\n",
        "        self.up3 = Up(256, 128 // factor, bilinear)\n",
        "        self.up4 = Up(128, 64, bilinear)\n",
        "        self.outc = OutConv(64, n_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.inc(x)\n",
        "        x2 = self.down1(x1)\n",
        "        x3 = self.down2(x2)\n",
        "        x4 = self.down3(x3)\n",
        "        x5, lv = self.down4(x4)\n",
        "        x = self.up1(x5, x4)\n",
        "        x = self.up2(x, x3)\n",
        "        x = self.up3(x, x2)\n",
        "        x = self.up4(x, x1)\n",
        "        logits = self.outc(x)\n",
        "        return logits, lv\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zuZRPX7RqpLD"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.optim import lr_scheduler\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import random\n",
        "import collections\n",
        "from torch.optim import lr_scheduler\n",
        "import pandas as pd\n",
        "import torch.nn.functional as F\n",
        "import pickle\n",
        "import argparse\n",
        "from U_net_denoising_autoencoder import UNet\n",
        "\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, images, label):\n",
        "        self.labels = label\n",
        "        self.images = images\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        X = self.images[index]\n",
        "        y = self.labels[index]\n",
        "        return X, y\n",
        "\n",
        "\n",
        "def training(num_epochs, my_autoencoder, optimizer, criterion, train_loader, validation_loader, test_loader, save_root,\n",
        "            noise_factor):\n",
        "    print('Training............')\n",
        "    val_loss_list = np.array([])\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss_train = 0.0\n",
        "\n",
        "        my_autoencoder.train()\n",
        "        train_len = 0\n",
        "        for train_x_batch, train_y in train_loader:\n",
        "            train_x = Variable(train_x_batch).cuda()\n",
        "            train_x_noise = Variable(train_x_batch + noise_factor*torch.randn(train_x_batch.shape)).cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            train_output, train_lv = my_autoencoder(train_x_noise)\n",
        "            train_epoch_loss = criterion(train_output, train_x)\n",
        "\n",
        "            train_epoch_loss.backward()\n",
        "\n",
        "            epoch_loss_train += (train_epoch_loss.data.item() * len(train_x_batch))\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            train_len += len(train_x_batch)\n",
        "\n",
        "\n",
        "        train_loss = epoch_loss_train / train_len\n",
        "\n",
        "        with torch.no_grad():\n",
        "\n",
        "            epoch_loss_val = 0.0\n",
        "            epoch_loss_test = 0.0\n",
        "\n",
        "            my_autoencoder.eval()\n",
        "            val_len = 0\n",
        "            for validation_x_batch, validation_y_batch in validation_loader:\n",
        "                val_x = Variable(validation_x_batch).cuda()\n",
        "                val_x_noise = Variable(validation_x_batch + noise_factor*torch.randn(validation_x_batch.shape)).cuda()\n",
        "\n",
        "                val_output, val_lv = my_autoencoder(val_x_noise)\n",
        "                val_epoch_loss = criterion(val_output, val_x)\n",
        "\n",
        "                epoch_loss_val += (val_epoch_loss.data.item() * len(validation_x_batch))\n",
        "                val_len += len(validation_x_batch)\n",
        "\n",
        "            val_loss = epoch_loss_val / val_len\n",
        "\n",
        "            my_autoencoder.eval()\n",
        "            test_len = 0\n",
        "            for test_x_batch, test_y_batch in test_loader:\n",
        "\n",
        "                test_x = Variable(test_x_batch).cuda()\n",
        "                test_x_noise = Variable(test_x_batch + noise_factor*torch.randn(test_x_batch.shape)).cuda()\n",
        "\n",
        "                test_output, test_lv = my_autoencoder(test_x_noise)\n",
        "                test_epoch_loss = criterion(test_output, test_x)\n",
        "\n",
        "                epoch_loss_test += (test_epoch_loss.data.item() * len(test_x_batch))\n",
        "\n",
        "                test_len += len(test_x_batch)\n",
        "\n",
        "            test_loss = epoch_loss_test / test_len\n",
        "\n",
        "        val_loss_list = np.append(val_loss_list, val_loss)\n",
        "\n",
        "        if val_loss_list[epoch] == val_loss_list.min():\n",
        "            print('model_saving ----- epoch : {}, validation_loss : {:.6f}'.format(epoch, val_loss))\n",
        "            torch.save(my_autoencoder.state_dict(), save_root + '/U-net_checkpoint.pt')\n",
        "\n",
        "        if (epoch + 1) == 1 :\n",
        "            print('Epoch [{}/{}], Train loss : {:.4f}, val loss : {:.4f}, test loss : {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss, test_loss))\n",
        "\n",
        "        if (epoch + 1) % 10 == 0 :\n",
        "            print('Epoch [{}/{}], Train loss : {:.4f}, val loss : {:.4f}, test loss : {:.4f}'.format(epoch+1, num_epochs, train_loss, val_loss, test_loss))\n",
        "\n",
        "\n",
        "def test_result(my_autoencoder, train_loader, validation_loader, test_loader, criterion, save_root, noise_factor):\n",
        "    fname = os.path.join(save_root, 'U-net_checkpoint.pt')\n",
        "    checkpoint = torch.load(fname)\n",
        "    my_autoencoder.load_state_dict(checkpoint)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        epoch_loss_train = 0.0\n",
        "        train_input = np.array([]).reshape(0, 3, 128, 128)\n",
        "        train_noise = np.array([]).reshape(0, 3, 128, 128)\n",
        "        train_result = np.array([]).reshape(0, 3, 128, 128)\n",
        "        train_latent_variable = np.array([]).reshape(0, 512, 8, 8)\n",
        "        train_label = np.array([])\n",
        "\n",
        "        epoch_loss_val = 0.0\n",
        "        val_input = np.array([]).reshape(0, 3, 128, 128)\n",
        "        val_noise = np.array([]).reshape(0, 3, 128, 128)\n",
        "        val_result = np.array([]).reshape(0, 3, 128, 128)\n",
        "        val_latent_variable = np.array([]).reshape(0, 512, 8, 8)\n",
        "        val_label = np.array([])\n",
        "\n",
        "        epoch_loss_test = 0.0\n",
        "        test_input = np.array([]).reshape(0, 3, 128, 128)\n",
        "        test_noise = np.array([]).reshape(0, 3, 128, 128)\n",
        "        test_result = np.array([]).reshape(0, 3, 128, 128)\n",
        "        test_latent_variable = np.array([]).reshape(0, 512, 8, 8)\n",
        "        test_label = np.array([])\n",
        "\n",
        "        train_len = 0\n",
        "        for train_x_batch, train_y in train_loader:\n",
        "            train_x = Variable(train_x_batch).cuda()\n",
        "            train_x_noise = Variable(train_x_batch + noise_factor*torch.randn(train_x_batch.shape)).cuda()\n",
        "\n",
        "            train_output, train_lv = my_autoencoder(train_x_noise)\n",
        "            train_lv = train_lv.detach().cpu().numpy()\n",
        "            train_latent_variable = np.append(train_latent_variable, train_lv, axis = 0)\n",
        "            train_label = np.append(train_label, train_y)\n",
        "            train_epoch_loss = criterion(train_output, train_x)\n",
        "\n",
        "            train_input = np.append(train_input, train_x.data.cpu().numpy(), axis = 0)\n",
        "            train_noise = np.append(train_noise, train_x_noise.data.cpu().numpy(), axis = 0)\n",
        "            train_result = np.append(train_result, train_output.data.cpu().numpy(), axis = 0)\n",
        "\n",
        "            epoch_loss_train += (train_epoch_loss.data.item() * len(train_x_batch))\n",
        "\n",
        "            train_len += len(train_x_batch)\n",
        "\n",
        "        train_loss = epoch_loss_train / train_len\n",
        "\n",
        "        my_autoencoder.eval()\n",
        "        val_len = 0\n",
        "        for validation_x_batch, validation_y_batch in validation_loader:\n",
        "            val_x = Variable(validation_x_batch).cuda()\n",
        "            val_x_noise = Variable(validation_x_batch + noise_factor*torch.randn(validation_x_batch.shape)).cuda()\n",
        "\n",
        "            val_output, val_lv = my_autoencoder(val_x_noise)\n",
        "            val_lv = val_lv.detach().cpu().numpy()\n",
        "            val_latent_variable = np.append(val_latent_variable, val_lv, axis = 0)\n",
        "            val_label = np.append(val_label, validation_y_batch)\n",
        "            val_epoch_loss = criterion(val_output, val_x)\n",
        "\n",
        "            val_input = np.append(val_input, val_x.data.cpu().numpy(), axis = 0)\n",
        "            val_noise = np.append(val_noise, val_x_noise.data.cpu().detach(), axis = 0)\n",
        "            val_result = np.append(val_result, val_output.data.cpu().numpy(), axis = 0)\n",
        "\n",
        "            epoch_loss_val += (val_epoch_loss.data.item() * len(validation_x_batch))\n",
        "            val_len += len(validation_x_batch)\n",
        "\n",
        "        val_loss = epoch_loss_val / val_len\n",
        "\n",
        "        test_len = 0\n",
        "        for test_x_batch, test_y_batch in test_loader:\n",
        "\n",
        "            test_x = Variable(test_x_batch).cuda()\n",
        "            test_x_noise = Variable(test_x_batch + noise_factor*torch.randn(test_x_batch.shape)).cuda()\n",
        "\n",
        "            test_output, test_lv = my_autoencoder(test_x_noise)\n",
        "            test_lv = test_lv.detach().cpu().numpy()\n",
        "            test_latent_variable = np.append(test_latent_variable, test_lv, axis = 0)\n",
        "            test_label = np.append(test_label, test_y_batch)\n",
        "            test_epoch_loss = criterion(test_output, test_x)\n",
        "\n",
        "            epoch_loss_test += (test_epoch_loss.data.item() * len(test_x_batch))\n",
        "\n",
        "            test_input = np.append(test_input, test_x.data.cpu().numpy(), axis = 0)\n",
        "            test_noise = np.append(test_noise, test_x_noise.data.cpu().numpy(), axis = 0)\n",
        "            test_result = np.append(test_result, test_output.data.cpu().numpy(), axis = 0)\n",
        "\n",
        "            test_len += len(test_x_batch)\n",
        "\n",
        "        test_loss = epoch_loss_test / test_len\n",
        "\n",
        "    print('Train loss : {:.7f}, val loss : {:.7f}, test loss : {:.7f}'.format(train_loss, val_loss, test_loss))\n",
        "\n",
        "    np.save(save_root + '/training_latent_variable_noBatchNorm.npy', train_latent_variable)\n",
        "    np.save(save_root + '/training_latent_variable_stage_noBatchNorm.npy', train_label)\n",
        "    np.save(save_root + '/validation_latent_variable_noBatchNorm.npy', val_latent_variable)\n",
        "    np.save(save_root + '/validation_latent_variable_stage_noBatchNorm.npy', val_label)\n",
        "    np.save(save_root + '/test_latent_variable_noBatchNorm.npy', test_latent_variable)\n",
        "    np.save(save_root + '/test_latent_variable_stage_noBatchNorm.npy', test_label)\n",
        "\n",
        "    np.save(save_root + '/training_input.npy', train_input)\n",
        "    np.save(save_root + '/training_stage.npy', train_label)\n",
        "    np.save(save_root + '/validation_input.npy', val_input)\n",
        "    np.save(save_root + '/validation_stage.npy', val_label)\n",
        "    np.save(save_root + '/test_input.npy', test_input)\n",
        "    np.save(save_root + '/test_stage.npy', test_label)\n",
        "\n",
        "    np.save(save_root + '/training_noise.npy', train_noise)\n",
        "    np.save(save_root + '/validation_noise.npy', val_noise)\n",
        "    np.save(save_root + '/test_noise.npy', test_noise)\n",
        "\n",
        "    np.save(save_root + '/training_result.npy', train_result)\n",
        "    np.save(save_root + '/validation_input.npy', val_result)\n",
        "    np.save(save_root + '/test_result.npy', test_result)\n",
        "\n",
        "def main():\n",
        "\n",
        "    parser = argparse.ArgumentParser()\n",
        "\n",
        "    # Required parameters\n",
        "    parser.add_argument(\n",
        "        \"--train_image_root\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--train_stage_root\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--val_image_root\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--val_stage_root\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--test_image_root1\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--test_stage_root1\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--test_image_root2\",\n",
        "        default= None,\n",
        "        type=str,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--test_stage_root2\",\n",
        "        default= None,\n",
        "        type=str,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--independent_val\",\n",
        "        default='Yes',\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--save_root\",\n",
        "        default=None,\n",
        "        type=str,\n",
        "        required=True,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--batch_size\",\n",
        "        default=16,\n",
        "        type=int,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--noise_factor\",\n",
        "        default=0.1,\n",
        "        type=float,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--learning_rate\",\n",
        "        default=1e-2,\n",
        "        type=float,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "    parser.add_argument(\n",
        "        \"--num_epochs\",\n",
        "        default=3,\n",
        "        type=int,\n",
        "        required=False,\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    args = parser.parse_args(\"\")\n",
        "\n",
        "\n",
        "    if args.independent_val == 'Yes':\n",
        "        train_x_re = np.load(args.train_image_root)\n",
        "        train_y_re = np.load(args.train_stage_root)\n",
        "        validation_x = np.load(args.val_image_root)\n",
        "        validation_y = np.load(args.val_stage_root)\n",
        "\n",
        "        if args.test_image_root2 is not None:\n",
        "            test_x_before1 = np.load(args.test_image_root1)\n",
        "            test_y_before1 = np.load(args.test_stage_root1)\n",
        "            test_x_before2 = np.load(args.test_image_root2)\n",
        "            test_y_before2 = np.load(args.test_image_root2)\n",
        "            test_x_before = np.append(test_x_before1, test_x_before2, axis = 0)\n",
        "            test_y_before = np.append(test_y_before1, test_y_before2, axis = 0)\n",
        "        else:\n",
        "            test_x_before = np.load(args.test_image_root1)\n",
        "            test_y_before = np.load(args.test_stage_root1)\n",
        "\n",
        "    else:\n",
        "\n",
        "        train_x_before1 = np.load(args.train_image_root)\n",
        "        train_y_before1 = np.load(args.train_stage_root)\n",
        "        train_x_before2 = np.load(args.val_image_root)\n",
        "        train_y_before2 = np.load(args.val_stage_root)\n",
        "\n",
        "        if args.test_image_root2 is not None:\n",
        "            test_x_before1 = np.load(args.test_image_root1)\n",
        "            test_y_before1 = np.load(args.test_stage_root1)\n",
        "            test_x_before2 = np.load(args.test_image_root2)\n",
        "            test_y_before2 = np.load(args.test_stage_root2)\n",
        "            test_x_before = np.append(test_x_before1, test_x_before2, axis = 0)\n",
        "            test_y_before = np.append(test_y_before1, test_y_before2, axis = 0)\n",
        "        else:\n",
        "            test_x_before = np.load(args.test_image_root1)\n",
        "            test_y_before = np.load(args.test_stage_root1)\n",
        "\n",
        "        train_x_before = np.append(train_x_before1, train_x_before2, axis = 0)\n",
        "        train_y_before = np.append(train_y_before1, train_y_before2, axis = 0)\n",
        "\n",
        "        test_x_before = np.append(test_x_before1, test_x_before2, axis = 0)\n",
        "        test_y_before = np.append(test_y_before1, test_y_before2, axis = 0)\n",
        "\n",
        "        train_y_before = train_y_before.astype('int')\n",
        "        test_y_before = test_y_before.astype('int')\n",
        "\n",
        "        val_idx = [3, 12, 27, 52, 67, 79, 81, 88]\n",
        "        train_idx = [i for i in range(len(train_x_before)) if i not in val_idx]\n",
        "\n",
        "        train_x_re = train_x_before[train_idx]\n",
        "        train_y_re = train_y_before[train_idx]\n",
        "\n",
        "        validation_x = train_x_before[val_idx]\n",
        "        validation_y = train_y_before[val_idx]\n",
        "\n",
        "    train_x = train_x_re.astype('float16')\n",
        "    train_y = train_y_re\n",
        "    val_x = validation_x.astype('float16')\n",
        "    val_y = validation_y\n",
        "    test_x = test_x_before.astype('float16')\n",
        "    test_y = test_y_before\n",
        "\n",
        "    train_x_tr = torch.from_numpy(train_x).type(torch.FloatTensor)\n",
        "    train_y_tr = torch.from_numpy(train_y)\n",
        "    val_x_tr = torch.from_numpy(val_x).type(torch.FloatTensor)\n",
        "    val_y_tr = torch.from_numpy(val_y)\n",
        "    test_x_tr = torch.from_numpy(test_x).type(torch.FloatTensor)\n",
        "    test_y_tr = torch.from_numpy(test_y)\n",
        "\n",
        "    training_set = Dataset(train_x_tr, train_y_tr)\n",
        "    train_loader = DataLoader(training_set, batch_size = args.batch_size, shuffle=True)\n",
        "    batch_len_train = len(train_loader)\n",
        "\n",
        "    validation_set = Dataset(val_x_tr, val_y_tr)\n",
        "    validation_loader = DataLoader(validation_set, batch_size = args.batch_size, shuffle=True)\n",
        "    batch_len_val = len(validation_loader)\n",
        "\n",
        "    test_set = Dataset(test_x_tr, test_y_tr)\n",
        "    test_loader = DataLoader(test_set, batch_size = 1, shuffle = True)\n",
        "    batch_len_test = len(test_loader)\n",
        "\n",
        "    my_autoencoder = UNet(3, 3);\n",
        "    my_autoencoder.cuda();\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.SGD(my_autoencoder.parameters(),lr = args.learning_rate, momentum = 0.9)\n",
        "\n",
        "    training(args.num_epochs, my_autoencoder, optimizer, criterion, train_loader, validation_loader, test_loader, save_root = args.save_root,\n",
        "            noise_factor = args.noise_factor)\n",
        "    test_result(my_autoencoder, train_loader, validation_loader, test_loader, criterion, save_root = args.save_root,\n",
        "            noise_factor = args.noise_factor)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIA1WPvyqsh5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}